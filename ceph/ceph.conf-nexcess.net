[global]
fsid = 31c6c27c-4383-424e-ac30-8d7b08326903
mon initial members = ceph01-mon1.nexcess.net, ceph01-mon2.nexcess.net, ceph01-mon3.nexcess.net
mon host = 172.17.252.11, 172.17.252.12, 172.17.252.13
public network = 172.17.252.0/24
cluster network = 172.17.253.0/24
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
## osd journal size = {2 * (expected throughput * filestore max sync interval)}
osd journal size = 12500
## filestore xattr use omap = true {for ext4}
filestore xattr use omap = true
## osd pool default size = 3 {write object 3 times}
osd pool default size = 3
## osd pool default min size = 2 {can go down to 2 when in degraded state}
osd pool default min size = 2
## ceph docs recommend the closest power of 2 of this formula: total = (100 * {number of OSDs}) / {replication level}
osd pool default pg num = 2048
osd pool default pgp num = 2048
## distribute replicas across hosts, rather than osds (default)
osd crush chooseleaf type = 1
